# ğŸ tinybug

**Mutation Testing Platform for Measuring Test Effectiveness**

---

## ğŸ§  What is tinybug?

**tinybug** is a platform for **mutation testing** â€” a technique to **measure how effective your tests really are** at catching bugs.

Modern development teams often focus on code coverage, but **code coverage â‰  bug detection**. Tests might run the code, but do they actually validate the behavior? Many tests pass even when the code they're covering is broken â€” due to poor assertions, missing checks, or lack of scenario diversity.

Thatâ€™s where **tinybug** comes in.

## ğŸ’¡ What is Mutation Testing?

Mutation testing works by **injecting small, artificial bugs (mutations)** into your codebase â€” particularly in code thatâ€™s already covered by your tests â€” and then running the tests again.

If the tests **fail**, great â€” theyâ€™re doing their job.  
If the tests **still pass**, that suggests **a gap in your test effectiveness**.

It helps answer:  
**"If this code had a bug, would our tests even catch it?"**

More on mutation testing:
- [Wikipedia: Mutation Testing](https://en.wikipedia.org/wiki/Mutation_testing)
- [Beginnerâ€™s Guide (Case Lab)](https://medium.com/@case_lab/mutation-testing-a-beginners-guide-41b731dc601e)
- [Googleâ€™s Mutation Testing at Scale (2017)](https://research.google/pubs/state-of-mutation-testing-at-google/)
- [PIT (for Java)](https://pitest.org/)
- [Stryker (for JS, TS, and more)](https://stryker-mutator.io/docs/)

## ğŸ¤– Why Now?

Mutation testing has been around for decades, but hasnâ€™t seen broad adoption due to:
1. **Low baseline coverage** in many teams.
2. **High cost** â€” mutation testing multiplies test execution time.
3. **Lack of approachable tooling** â€” existing tools like [Stryker](https://stryker-mutator.io) are powerful, but onboarding and usage at scale can be rough.

But the landscape is changing:
- **Code coverage is improving** across the industry.
- **Test automation is easier** than ever with modern CI/CD.
- **AI can now generate tests** â€” but how do we trust *those* tests?

If AI writes your tests, how do you know if *they're any good*?

Mutation testing is one of the best ways to evaluate **AI-generated tests** â€” by seeing whether they can detect meaningful failures in code.

## ğŸ¯ Goal of tinybug

To provide a mutation testing platform that is:

- ğŸª¶ **Lightweight** â€“ minimal setup, easy integration  
- ğŸ§ª **Test-focused** â€“ measure what your tests *actually* validate  
- ğŸ” **Insightful** â€“ useful, actionable reports  
- ğŸ’» **AI-relevant** â€“ help assess the quality of AI-generated tests  

## ğŸ”§ Status

This is an early-stage, work-in-progress project.

Stay tuned as we build out:
- Mutation operator framework
- Test orchestration and result tracking
- Mutation-aware reporting
- CI integration
- Dashboard & analytics

## ğŸ“„ License

MIT
